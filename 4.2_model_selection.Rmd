---
title: "Выбор моделей"
author: Марина Варфоломеева, Вадим Хайтов
output:
  ioslides_presentation:
    widescreen: true
    css: assets/my_styles.css
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
# output options
options(width = 70, scipen = 6, digits = 3)
library(knitr)
# chunk default options
opts_chunk$set(fig.align='center', tidy = FALSE, fig.width = 7, fig.height = 3, warning = FALSE)
```

## Мы рассмотрим

- Принципы выбора лучшей линейной модели
- Сравнение линейных моделей разными способами:
    - частный F-критерий
    - тесты отношения правдоподобий
    - информационные критерии AIC и BIC

```{r, echo=FALSE, message=FALSE, purl=FALSE}
library(ggplot2)
theme_set(theme_bw())
library(gridExtra)
```

# Принципы выбора лучшей линейной модели

"Essentially, all models are wrong,  
but some are useful"  
Georg E. P. Box


## Важно не только тестирование гипотез, но и построение моделей

- Проверка соответствия наблюдаемых данных предполагаемой функциональной связи между зависимой перменной и предикторами:
    - оценки параметров,
    - __тестирование гипотез__,
    - оценка объясненной изменчивости ($R^2$),
    - анализ остатков 

- __Построение моделей__ для предсказания значений в новых условиях:
    - Выбор оптимальной модели
    - Oценка предсказательной способности модели

## Какую модель можно подобрать для описания этой закономерности?

```{r, echo=FALSE, fig.height=4, fig.width=3, purl=FALSE}
library(ggplot2)
n <- 10
set.seed(384)
x <- rnorm(n, 4, 1.2)
y <- 10 + 0.59*x  + 0.1*x^2+ 0.001425*x^3 + rnorm(n)

lc <- coef(lm(y ~ x))
cc <- coef(lm(y ~ poly(x, 3, raw = TRUE)))
fic <- coef(lm(y ~ poly(x, 5, raw = TRUE)))

lin <- function(x){lc[1] + lc[2]*x}
cub <- function(x){cc[1] + cc[2]*x + cc[3]*x^2 + cc[4]*x^3}
fif <- function(x){fic[1] + fic[2]*x + fic[3]*x^2 + fic[4]*x^3 + fic[5]*x^4 + fic[6]*x^5}

lm_eqn = function(coeffs){
if(length(coeffs) == 2) {
eq <- substitute(italic(y) == a + b %.% italic(x),
                 list(a = format(coeffs[1], digits = 2),
                      b = format(coeffs[2], digits = 2)))
}
if(length(coeffs) == 4) {
  eq <- substitute(italic(y) == a + b %.% italic(x) + c %.% italic(x)^2 + d %.% italic(x)^3,
                 list(a = format(coeffs[1], digits = 2),
                      b = format(coeffs[2], digits = 2),
                      c = format(coeffs[3], digits = 2),
                      d = format(coeffs[4], digits = 2)))
}
if(length(coeffs) == 6) {
  eq <- substitute(italic(y) == a + b %.% italic(x) + c %.% italic(x)^2 + d %.% italic(x)^3 + e %.% italic(x)^4 + f %.% italic(x)^5,
                 list(a = format(coeffs[1], digits = 2),
                      b = format(coeffs[2], digits = 2),
                      c = format(coeffs[3], digits = 2),
                      d = format(coeffs[4], digits = 2),
                      e = format(coeffs[5], digits = 2),
                      f = format(coeffs[6], digits = 2)))
}
  as.character(as.expression(eq))
  }

library(grid)

pp <- ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) + geom_point() + theme(plot.title = element_text(size = 10), plot.margin = unit(c(0.5, 0.5, 0.1, 0.1), "lines"))

under <- pp + stat_function(fun = lin, colour = "blue") +  annotate("text", x=1, y=18.2, label=lm_eqn(lc), hjust=0, size=3, family="Times", fontface="italic", parse=TRUE)
right <- pp + stat_function(fun = cub, colour = "blue")   + annotate("text", x=1, y=18.2, label=lm_eqn(cc), hjust=0, size=3, family="Times", fontface="italic", parse=TRUE)
over <- pp + stat_function(fun = fif, colour = "blue")  + annotate("text", x=1, y=18.2, label=lm_eqn(fic), hjust=0, size=3, family="Times", fontface="italic", parse=TRUE)

pp
```

## Начнем с линейной модели

Все ли хорошо с подобранной моделью?

```{r models-lin, echo=FALSE, fig.height=4, fig.width=3, purl=FALSE}
pp_lm <- pp + geom_smooth(method = "lm", se = FALSE)
pp_lm
```

## Для этих данных можно подобрать несколько моделей

Какая из этих моделей лучше описывает данные?

```{r models-no-labs, echo=FALSE, fig.height=3.5, fig.width=10, purl=FALSE}
grid.arrange(under,
             right,
             over,
             ncol = 3)
```

## Для этих данных можно подобрать несколько моделей


```{r models-labs, echo=FALSE, fig.height=3.5, fig.width=10, purl=FALSE}
grid.arrange(under + labs(title = "Высокая погрешность \n(недообучение)"),
             right + labs(title = "Оптимальая \nмодель"),
             over + labs(title = "Высокая дисперсия \n(переобучение)"),
             ncol = 3)
```
В недообученной (underfitted) модели слишком мало параметров, ее предсказания неточны.   

В переобученной (overfitted) модели слишком много параметров, она предсказывает еще и случайный шум. 

## Последствия переобучения модели

Переобучение происходит, когда модель из-за избыточного усложнения описывает уже не только отношения между переменными, но и случайный шум

При увеличении числа предикторов в модели:

- более точное описание данных, по которым подобрана модель   
- низкая точность предсказаний на новых данных из-за переобучения.   

## При постоении моделей важно помнить о трех типах дисперсии

```{r, echo=FALSE, fig.height=5, fig.width=9, purl=FALSE}
library(readxl)
cat <- read_excel("data/catsM.xlsx", sheet = 1)

cat_model <- lm(Hwt ~ Bwt, data = cat)
cat_predicted <- predict(cat_model, interval="prediction")
cat_predicted <- data.frame(cat, cat_predicted)

pl_cat <- ggplot(cat, aes(x = Bwt, y = Hwt)) + geom_point() + theme(plot.title = element_text(size = 10), axis.text = element_text(size = 8), axis.title = element_text(size = 8) ) + ggtitle("Model") + geom_smooth(method = "lm", se = F) + labs(x = "X", y = "Y")

pl_exp <- pl_cat + geom_smooth(method="lm", se=F, size=1.3) + geom_abline(aes(intercept=mean(Hwt), slope=0), size=1.3) + geom_text(label="Mean", aes(x=2.2, y=(mean(Hwt)+0.5)), size = 3) + geom_segment(data=cat_predicted, aes(x=Bwt, y=mean(Hwt), xend=Bwt, yend=fit)) + ggtitle("Explained variation") 
 

pl_res <- pl_cat + geom_smooth(method="lm", se=F, size=1.3) + geom_segment(data=cat_predicted, aes(x=Bwt, y=Hwt, xend=Bwt, yend=fit)) + ggtitle("Residual variation")

pl_tot <-pl_cat + geom_abline(aes(intercept=mean(Hwt), slope=0), size=1.3) + geom_text(label="Mean", aes(x=2.2, y=(mean(Hwt)+0.5)), size = 3) + geom_segment(data=cat_predicted, aes(x=Bwt, y=Hwt, xend=Bwt, yend=mean(Hwt))) + ggtitle("Total variation")

grid.arrange(pl_cat, pl_exp, pl_res, pl_tot, nrow=1)
```

## При постоении моделей важно помнить о трех типах дисперсии


|**Объясненная дисперсия**  | **Остаточная дисперсия**   | **Полная дисперсия** | 
|-----|-----|-----|
| $SS_{Regression}=\sum{(\hat{y}-\bar{y})^2}$ | $SS_{Residual}=\sum{(\hat{y}-y_i)^2}$ | $SS_{Total}=\sum{(\bar{y}-y_i)^2}$ |
| $df_{Regression} = 1$ | $df_{Residual} = n-2$  | $df_{Total} = n-1$ |
| $MS_{Regression} =\frac{SS_{Regression}}{df}$ | $MS_{Residual} =\frac{SS_{Residual}}{df_{Residual}}$ | $MS_{Total} =\frac{SS_{Total}}{df_{Total}}$ |




## Компромисс при подборе оптимальной модели:<br />точность vs. описание шума

### Хорошее описание существующих данных

Полный набор переменных:

- большая объясненная изменчивость ($R^2$),
- маленькая остаточная изменчивость ($MS_{Residual}$)
- большие стандартные ошибки
- сложная интерпертация


### Принцип парсимонии

_Entia non sunt multiplicanda praeter necessitatem_

Минимальный набор переменных, который может объяснить существующие данные:

- объясненная изменчивость меньше ($R^2$),
- остаточная изменчивость больше ($MS_{Residual}$)
- стандартные ошибки меньше
- интерпертация проще

## Критерии и методы выбора моделей зависят от задачи

### _Объяснение закономерностей_

- Нужны точные тесты влияния предикторов: F-тесты или тесты отношения правдоподобий (likelihood-ratio tests)

### _Описание функциональной зависимости_

- Нужна точность оценки параметров

### _Предсказание значений зависимой переменной_

- Парсимония: "информационные" критерии (АIC, BIC, AICc, QAIC, и т.д.)
- Нужна оценка качества модели на данных, которые не использовались для ее первоначальной подгонки: методы ресамплинга (кросс-валидация, бутстреп)

## Дополнительные критерии для сравнения моделей:

### Не позволяйте компьютеру думать за вас!

- Хорошая модель должна соответствовать условиям применимости

- Другие соображения: разумность, целесообразность модели, простота, ценность выводов, важность предикторов.


# Знакомство с данными для множественной линейной регрессии

## Пример: рак предстательной железы

От каких характеристик зависит концентрация простат-специфического антигена? (Stamey et al. 1989)

Переменных много, мы хотим из них выбрать оптимальный небольшой набор

97 пациентов:

- `lcavol` --- логарифм объема опухоли
- `lweight` --- логарифм веса
- `age` --- возраст пациента
- `lbph` --- логарифм степени доброкачественной гиперплазии
- `svi` --- поражение семенных пузырьков
- `lcp` --- логарифм меры поражения капсулы
- `gleason` --- оценка по шкале Глисона
- `pgg45` --- доля оценок 4 и 5 по шкале Глисона
- `lpsa` --- логарифм концентрации простат-специфичного антигена


## Что мы знаем про эти данные?

```{r}
prost <- read_excel("data/Prostate.xlsx")
str(prost)
colSums(is.na(prost))
```

## Есть два дискретных фактора, временно не будем их учитывать (пока не научимся)

```{r fig.width=10, fig.height=5}
pairs(prost)
```

## Проверяем на колинеарность предикторов

```{r}
M1 <- lm(lpsa ~ . - svi - gleason, data = prost)
library(car)
vif(M1) # При желании, можно удалить lcp, но мы не будем
```

## Выбросы, график остатков, квантильный график

```{r fig.width=10, fig.show='hold'}
set.seed(893247)
cook_cutoff <- 4 / (nrow(prost) - length(coef(M1) - 2))
op <- par(mfrow = c(1, 3)) # располагаем картинки в 3 колонки
plot(M1, which = 4, cook.levels = cook_cutoff)  # Расстояние Кука
residualPlot(M1)        # График остатков
qqPlot(M1)              # Квантильный график остатков
par(op)
```

>- Нелинейный паттерн в остатках? Возможно, нам нужно подобрать GAM? Или это неучтенные факторы?

## Заготовка графика остатков в ggplot

```{r}
M1_diag <- data.frame(prost, 
                      .resid = resid(M1, type = "pearson"),
                      .fitted = fitted(M1))
gg_res <- ggplot(M1_diag, aes(x = .fitted, y = .resid)) + 
  geom_point() + geom_smooth(method = "loess") +
  geom_hline(yintercept = 0)
gg_res
```

>- Выглядит подозрительно нелинейно


## Графики остатков от предикторов в модели и нет 

```{r eval=FALSE}
# Предикторы в модели lcavol, lweight, age, lbph, lcp, pgg45
gg_res %+% aes(x = lcavol)
gg_res %+% aes(x = lweight)
gg_res %+% aes(x = age)  #
gg_res %+% aes(x = lbph) #
gg_res %+% aes(x = lcp)
gg_res %+% aes(x = pgg45 ) # нелинейый паттерн
```

## Графики остатков от предикторов в модели и нет 

```{r echo=FALSE, fig.height=6, fig.width=10, purl=FALSE}
library(gridExtra)
grid.arrange(
# Предикторы в модели lcavol, lweight, age, lbph, lcp, pgg45
gg_res %+% aes(x = lcavol),
gg_res %+% aes(x = lweight),
gg_res %+% aes(x = age),
gg_res %+% aes(x = lbph), 
gg_res %+% aes(x = lcp),
gg_res %+% aes(x = pgg45 ), # нелинейый паттерн
ncol = 2)
```

## Графики остатков от предикторов не в модели 

```{r eval=FALSE}
# Дискретные предикторы не в модели svi, gleason
gg_res %+% aes(x = factor(svi)) + geom_boxplot()
gg_res %+% aes(x = factor(gleason)) + geom_boxplot()
# Похоже, что есть зависимость
# Возможно, добавление этих факторов в модель улучшит дело --- в следующий раз
```

```{r echo=FALSE, purl=FALSE}
grid.arrange(
# Дискретные предикторы не в модели svi, gleason
gg_res %+% aes(x = factor(svi)) + geom_boxplot(),
gg_res %+% aes(x = factor(gleason)) + geom_boxplot(),
ncol = 2)
# Похоже, что есть зависимость
# Возможно, добавление этих факторов в модель улучшит дело --- в следующий раз
```

## Влияют ли предикторы?

```{r}
summary(M1)
```

Не все предикторы влияют, возможно эту модель можно оптимизировать...

# Сравнение линейных моделей

# Вложенные модели (nested models)

## Вложенные модели (nested models)

Две модели являются _вложенными_, если одну из них можно получить из другой путем удаления некоторых предикторов.   

Удаление предиктора  - коэффициент при данном предикторе равен нулю. 

### Полная модель (full model)

М1: $y _i = \beta _0 + \beta _1 x _1 + \beta _2 x _2 + \epsilon _i$

### Неполные модели (reduced models)

М2: $y _i = \beta _0 + \beta _1 x _1 + \epsilon _i$   

М3: $y _i = \beta _0 + \beta _2 x _2 + \epsilon _i$

M2 вложена в M1   
M3 вложена в M1   
M2 и M3 не вложены друг в друга

### Нулевая модель (null model), вложена в полную (M1) и в неполные (M2, M3)

$y _i = \beta _0 + \epsilon _i$

## Задание

Для тренировки запишем вложенные модели для данной полной модели

(1)$y _i = \beta _0 + \beta _1 x _1 + \beta _2 x _2 + \beta _3 x _3 + \epsilon _i$

## Решение

Для тренировки запишем вложенные модели для данной полной модели

(1)$y _i = \beta _0 + \beta _1 x _1 + \beta _2 x _2 + \beta _3 x _3 + \epsilon _i$

<div class="columns-2">

Модели:

- (2)$y _i = \beta _0 + \beta _1 x _1 + \beta _2 x _2 + \epsilon _i$
- (3)$y _i = \beta _0 + \beta _1 x _1 + \beta _3 x _3 + \epsilon _i$
- (4)$y _i = \beta _0 + \beta _2 x _2 + \beta _3 x _3 + \epsilon _i$
- (5)$y _i = \beta _0 + \beta _1 x _1 + \epsilon _i$
- (6)$y _i = \beta _0 + \beta _2 x _2 + \epsilon _i$
- (7)$y _i = \beta _0 + \beta _3 x _3 + \epsilon _i$
- (8)$y _i = \beta _0 + \epsilon _i$<br /><br />

Вложенность:

- (2)-(4)- вложены в (1)<br /><br /><br />
- (5)-(7)- вложены в (1), при этом 
   - (5)вложена в (1), (2), (3); 
   - (6)вложена в (1), (2), (4); 
   - (7)вложена в (1), (3), (4)<br /><br />
- (8)- нулевая модель - вложена во все

</div>

# Частный F-критерий

## Сравнение вложенных линейных моделей при помощи F-критерия

### Полная модель 

$y _i = \beta _0 + \beta _1 x _{i1} + ... + \beta _k x _{ik} + \beta _p x _{ip} + \epsilon _i$  
$df _{reduced, full} = p$  
$df _{error, full} = n - p - 1$

### Уменьшеная модель  (без фактора $\beta _p x _{ip}$)

$y _i = \beta _0 + \beta _1 x _{i1} + ... + \beta _k x _{ik} + \epsilon _i$  
$df _{reduced, reduced} = k$  
$df _{error, reduced} = n - k - 1$

### Как оценить насколько больше изменчивости объясняет полная модель, чем уменьшенная модель?

>- Разница объясненной изменчивости --- $SS _{error,reduced} - SS _{error,full}$ 
>- С чем, по аналогии с обычным F-критерием, можно сравнить эту разницу объясненной изменчивости?
>- Можно сравнить с остаточной изменчивостью полной модели --- $SS _{error, full}$

## Сравнение вложенных линейных моделей при помощи F-критерия

### Полная модель 

$y _i = \beta _0 + \beta _1 x _{i1} + ... + \beta _k x _{ik} + \beta _p x _{ip} + \epsilon _i$  
$df _{reduced, full} = p$  
$df _{error, full} = n - p - 1$

### Уменьшеная модель  (без фактора $\beta _p x _{ip}$)

$y _i = \beta _0 + \beta _1 x _{i1} + ... + \beta _k x _{ik} + \epsilon _i$  
$df _{reduced, reduced} = k$  
$df _{error, reduced} = n - k - 1$

### Частный F-критерий - оценивает выигрыш объясненной дисперсии от включения фактора в модель

$$F = \frac {(SS _{error,reduced} - SS _{error,full}) / (df _{reduced, full} - df _{reduced, reduced})} {(SS _{error, full})/ df _{error, full}}$$

## Сравнение линейных моделей при помощи частного F-критерия

Постепенно удаляем предикторы. Модели обязательно должны быть вложенными! *

### Обратный пошаговый алгоритм (backward selection)

>- - 1.Подбираем полную модель

>- Повторяем 2-3 для каждого из предикторов:  
- 2.Удаляем один предиктор (строим уменьшенную модель)  
- 3.Тестируем отличие уменьшенной модели от полной

>- 4.Выбираем предиктор для окончательного удаления: это предиктор, удаление которого минимально ухудшает модель. Модель без него будет "полной" для следующего раунда выбора оптимальной модели.  

>- Повторяем 1-4 до тех пор, пока что-то можно удалить.

<hr/>
* --- __Важно!__ Начинать упрощать модель нужно со взаимодействий между предикторами.  Если взаимодействие из модели удалить нельзя, то нельзя удалять и отдельно стоящие предикторы, из которых оно состоит.

# Частный F-критерий в R

## Влияют ли предикторы?

>- Незначимо влияние age, lbph, lcp, pgg45. Можем попробовать оптимизировать модель

```{r}
summary(M1)
```

## Частный F-критерий, 1 способ: `anova(модель_1, модель_2)`

Вручную выполняем все действия и выбираем, что можно выкинуть, и так много раз.

```{r eval=FALSE}
M2 <- update(M1, . ~ . - age)
anova(M1, M2)
M3 <- update(M1, . ~ . - lbph)
anova(M1, M3)
M4 <- update(M1, . ~ . - lcp)
anova(M1, M4)
M5 <- update(M1, . ~ . - pgg45)
anova(M1, M5)
# Удаляем lcp, и потом повторяем все снова...
```

Но мы пойдем другим путем


##  Частный F-критерий, 2 способ: `drop1()`

Вручную тестировать каждый предиктор с помощью `anova()` слишком долго. Можно протестировать все за один раз при помощи `drop1()`

```{r}
drop1(M1, test = "F")
# Нужно убрать lcp
```

## Тестируем предикторы (шаг 2)

```{r}
# Убрали lcp
M2 <- update(M1, . ~ . - lcp)
drop1(M2, test = "F")
# Нужно убрать lbph 
```

## Тестируем предикторы (шаг 3)

```{r purl=FALSE}
# Убрали lbph 
M3 <- update(M2, . ~ . - lbph )
drop1(M3, test = "F")
# Нужно убрать age
```

## Тестируем предикторы (шаг 4)

```{r purl=FALSE}
# Убрали age
M4 <- update(M3, . ~ . - age)
drop1(M4, test = "F")
# Нужно убрать pgg45
```

## Тестируем предикторы (шаг 5)

```{r purl=FALSE}
# Убрали pgg45
M5 <- update(M4, . ~ . - pgg45)
drop1(M5, test = "F")
# Больше убрать ничего не получается
```

## Итоговая модель

```{r}
summary(M5)
```

## Задание

Проверьте финальную модель на выполнение условий применимости

## Выбросы, график остатков, квантильный график

```{r fig.width=10, fig.show='hold', purl=FALSE}
set.seed(293234)
cook_cutoff <- 4 / (nrow(prost) - length(coef(M5) - 2))
op <- par(mfrow = c(1, 3)) # располагаем картинки в 3 колонки
plot(M5, which = 4, cook.levels = cook_cutoff)  # Расстояние Кука
residualPlot(M5)        # График остатков
qqPlot(M5)              # Квантильный график остатков
par(op)
```

## График остатков --- заготовка

```{r purl=FALSE}
M_diag <- data.frame(prost, 
                      .resid = resid(M5, type = "pearson"),
                      .fitted = fitted(M5))
gg_res <- ggplot(M_diag, aes(x = .fitted, y = .resid)) + 
  geom_point() + geom_smooth(method = "loess") +
  geom_hline(yintercept = 0)
gg_res
```

>- Видны нелинейные паттерны. Нужно использовать другую модель! Но мы пока продолжим, чтобы поучиться.

## Графики остатков от предикторов в модели

```{r eval=FALSE, purl=FALSE}
# Предикторы в модели lcavol, lcp
gg_res %+% aes(x = lcavol)
gg_res %+% aes(x = lweight)  # нелинейый паттерн, возможно нужен GAM
```

```{r echo=FALSE, purl=FALSE}
grid.arrange(
# Предикторы в модели lcavol, lcp
gg_res %+% aes(x = lcavol),
gg_res %+% aes(x = lweight),  # нелинейый паттерн, возможно нужен GAM
ncol = 2
)
```

## Графики остатков от непрерывных предикторов не в модели
```{r eval=FALSE, purl=FALSE}
# Непрерывные предикторы не в модели lweight, age, lbph, pgg45, 
gg_res %+% aes(x = age)  #
gg_res %+% aes(x = lbph) #
gg_res %+% aes(x = lcp)    # нелинейый паттерн
gg_res %+% aes(x = pgg45 ) # нелинейый паттерн
```

```{r echo=FALSE, fig.height=4, purl=FALSE}
grid.arrange(
# Непрерывные предикторы не в модели lweight, age, lbph, pgg45, 
gg_res %+% aes(x = age),  #
gg_res %+% aes(x = lbph), #
gg_res %+% aes(x = lcp),   # нелинейый паттерн
gg_res %+% aes(x = pgg45), # нелинейый паттерн
ncol = 2
)
```


## Графики остатков от дискретных предикторов не в модели

```{r eval=FALSE, purl=FALSE}
# Дискретные предикторы не в модели svi, gleason
gg_res %+% aes(x = factor(svi)) + geom_boxplot()
gg_res %+% aes(x = factor(gleason)) + geom_boxplot()
# Есть зависимости. Возможно, добавление этих факторов в модель улучшит дело - в следующий раз
```

```{r echo=FALSE, purl=FALSE}
grid.arrange(
# Дискретные предикторы не в модели svi, gleason
gg_res %+% aes(x = factor(svi)) + geom_boxplot(),
gg_res %+% aes(x = factor(gleason)) + geom_boxplot(),
ncol = 2)
# Есть зависимости. Возможно, добавление этих факторов в модель улучшит дело - в следующий раз
```

## Описываем финальную модель

```{r}
summary(M5)
```

# Тесты отношения правдоподобий

## Вероятность и правдоподобие

Правдоподобие (likelihood) ---  способ измерить соотверствие имеющихся данных тому, что можно получить при определенных значениях параметров модели.

Мы оцениваем это как произведение вероятностей получения каждой из точек данных

$$L(\theta| data) = \Pi^n _{i = 1}f(x| \theta)$$

где $f(data| \theta)$ - функция плотности распределения с параметрами $\theta$

```{r gg-norm-tunnel, echo=FALSE, fig.height=4, purl=FALSE}
## Based on code by Arthur Charpentier:
## http://freakonometrics.hypotheses.org/9593
## TODO: wrap it into a function and adapt it for use with other distributions
## as Markus Gesmann has done here
## http://www.magesblog.com/2015/08/visualising-theoretical-distributions.html

op <- par(mar = c(0, 0, 0, 0))
n <- 2
X <- cat$Bwt 
Y <- cat$Hwt
df <- data.frame(X,Y)

# regression
reggig <- glm(Y ~ X, data = df, family = gaussian(link = "identity"))

# empty plot
vX <- seq(min(X) - 0.1, max(X) + 0.1, length = n)
vY <- seq(min(Y) - 10, max(Y) + 10, length = n)
mat <- persp(x = vX, y = vY, z = matrix(0, n, n), 
             zlim = c(0, 0.1),
             theta =  -30, phi = 20, expand = 0.1,
             ticktype  = "detailed",  box = FALSE, border = "gray60")

x <- seq(min(X), max(X), length = 501)

# expected values
C <- trans3d(x, predict(reggig, newdata = data.frame(X = x), type = "response"), rep(0, length(x)), mat)
lines(C, lwd = 2)

sdgig <- sqrt(summary(reggig)$dispersion)

# 1SD
y1 <- qnorm(.95, predict(reggig, newdata = data.frame(X = x), type = "response"),  sdgig)
C <- trans3d(x, y1, rep(0, length(x)), mat)
lines(C, lty = 2, col = "#d95f02")
y2 <- qnorm(.05, predict(reggig, newdata = data.frame(X = x), type = "response"),  sdgig)
C <- trans3d(x, y2, rep(0, length(x)), mat)
lines(C, lty = 2, col = "#d95f02")

# C <- trans3d(c(x, rev(x)), c(y1, rev(y2)), rep(0, 2 * length(x)), mat)
# polygon(C, border = NA, col = "yellow")

# data points
C <- trans3d(X, Y, rep(0, length(X)), mat)
points(C, pch = 1, col = "black", cex = 0.4)

# density curves
n <- 6
vX <- seq(min(X), max(X), length = n)

mgig <- predict(reggig, newdata = data.frame(X = vX))

sdgig <- sqrt(summary(reggig)$dispersion)

for(j in n:1){
  stp <- 251
  x <- rep(vX[j], stp)
  y <- seq(min(min(Y) - 10, 
               qnorm(.05, 
                     predict(reggig, 
                             newdata = data.frame(X = vX[j]), 
                             type = "response"),  
                     sdgig)), 
           max(Y) + 10, 
           length = stp)
  z0 <- rep(0, stp)
  z <- dnorm(y,  mgig[j],  sdgig)
  C <- trans3d(c(x, x), c(y, rev(y)), c(z, z0), mat)
  polygon(C, border = NA, col = "light blue", density = 40)
  C <- trans3d(x, y, z0, mat)
  lines(C, lty = 2, col = "grey60")
  C <- trans3d(x, y, z, mat)
  lines(C, col = "steelblue")
}
par(op)
```


## Выводим формулу правдоподобия для линейной модели с нормальным распределением ошибок

$y_i = \beta_0 + \beta_1x_1 + ... + \beta_kx_k + \epsilon_i$

Пусть в нашей модели остатки нормально распределены ($\epsilon_i \sim N(0, \sigma^2)$) и их значения независимы друг от друга:

$N(\epsilon_i; 0, \sigma^2) = \frac {1} { \sqrt {2\pi\sigma^2} } exp (-\frac {1} {2 \sigma^2} \epsilon_i^2)$

Функцию правдоподобия (likelihood, вероятность получения нашего набора данных) можно записать как произведение вероятностей:

$L(\epsilon_i|\mathbf{y}, \mathbf{x}) = \Pi^n _{n = 1} N(\epsilon_i, \sigma^2) = \frac {1} {\sqrt{(2\pi\sigma^2)^n}} exp(- \frac {1} {2\sigma^2} \sum {\epsilon_i}^2)$

Поскольку $\epsilon_i = y_i - (\beta_0 + \beta_1x_1 + ... + \beta_kx_k)$

то функцию правдоподобия можно переписать так:

$L(\beta_1...\beta_k, \sigma^2| \mathbf{y}, \mathbf{x}) = \frac {1} {\sqrt{(2\pi\sigma^2)^n}}exp(- \frac {1} {2\sigma^2} \sum (y_i - (\beta_0 + \beta_1x_1 + ... + \beta_kx_k))^2)$

## Подбор параметров модели методом максимального правдоподобия

Чтобы найти параметры модели

$$y_i = \beta_0 + \beta_1x_1 + ... + \beta_kx_k + \epsilon_i$$

нужно найти такое сочетание параметров  $\beta_0$, $\beta_1$, ... $\beta_k$, и $\sigma^2$, при котором функция правдоподобия будет иметь максимум:

$\begin{array}{l}
L(\beta_1...\beta_k, \sigma^2| \mathbf{y}, \mathbf{x}) &= \frac {1} {\sqrt{(2\pi\sigma^2)^n}} exp(- \frac {1} {2\sigma^2} \sum {\epsilon_i}^2) = \\
&= \frac {1} {\sqrt{(2\pi\sigma^2)^n}}exp(- \frac {1} {2\sigma^2} \sum (y_i - (\beta_0 + \beta_1x_1 + ... ...\beta_kx_k))^2)
\end{array}$


## Логарифм правдоподобия (loglikelihood)

Вычислительно проще работать с логарифмами правдоподобий (loglikelihood)

Если функция правдоподобия

$\begin{array}{l}
L(\beta_1...\beta_k, \sigma^2| \mathbf{y}, \mathbf{x}) &= \frac {1} {\sqrt{(2\pi\sigma^2)^n}} exp(- \frac {1} {2\sigma^2} \sum {\epsilon_i}^2) = \\
&= \frac {1} {\sqrt{(2\pi\sigma^2)^n}}exp(- \frac {1} {2\sigma^2} \sum (y_i - (\beta_0 + \beta_1x_1 + ... + \beta_kx_k))^2)
\end{array}$

то логарифм правдоподобия

$\begin{array}{l}
logLik(\beta_1...\beta_k, \sigma^2| \mathbf{y}, \mathbf{x}) &= & \\
ln L(\beta_1...\beta_k, \sigma^2| \mathbf{y}, \mathbf{x}) &= &- \frac{n}{2} (ln2\pi + ln\sigma^2) - \frac{1}{2\sigma^2}(\sum \epsilon^2_i) = \\
&= &- \frac{n}{2} (ln2\pi + ln\sigma^2) - \\
& &- \frac{1}{2\sigma^2}(\sum (y_i - (\beta_0 + \beta_1x_1 + ... + \beta_kx_k))^2)
\end{array}$

Чем больше логарифм правдоподобия тем лучше модель

## Подбор параметров модели методом максимального правдоподобия

Для подбора параметров методом максимального правдоподобия используют функцию `glm()`


```{r}
# Симулированные данные
set.seed(9328)
dat <- data.frame(X = runif(n = 50, min = 0, max = 10))
dat$Y <- 3 + 15 * dat$X + rnorm(n = 50, mean = 0, sd = 1)

# Подбор модели двумя способами
Mod     <-  lm(Y ~ X, data = dat) # МНК
Mod_glm <- glm(Y ~ X, data = dat) # МЛ

# Одинаковые оценки коэффициентов
coefficients(Mod)
coefficients(Mod_glm)
```


## Логарифм правдоподобия

$LogLik$ для модели можно найти с помощью функции `logLic()`

```{r}
logLik(Mod_glm)
```

## Логарифм правдоподобия вручную


```{r}
# Предсказанные значения
dat$predicted <- predict(Mod)
# Оценка дисперсии
SD <- summary(Mod)$sigma 
# Вероятности для каждой точки
dat$Prob <- dnorm(dat$Y, mean = dat$predicted, sd = SD)
# Логарифм вероятностей
dat$LogProb <- log(dat$Prob)
# Логарифм произведения, равный сумме логарифмов
sum(dat$LogProb)
```



## Тест отношения правдоподобий (Likelihood Ratio Test)

Тест отношения правдоподобий позволяет определить какая модель более правдоподобна с учетом данных.

$$LRT = 2ln\Big(\frac{L_1}{L_2}\Big) = 2(lnL_1 - lnL_2)$$

- $L_1$, $L_2$ - правдоподобия полной и уменьшеной модели
- $lnL_1$, $lnL_2$ - логарифмы правдоподобий

Разница логарифмов правдоподобий имеет распределение $\chi^2$ с числом степеней свободы $df = df_2 - df_1$

# Тест отношения правдоподобий в R

## Задание

Для этой полной модели

```{r}
GLM1 <- glm(lpsa ~ . - svi - gleason, data = prost)
```

Подберите оптимальную модель при помощи тестов отношения правдоподобий

Тест отношения правдоподобий можно сделать с помощью тех же функций, что и частный F-критерий:

- по-одному `anova(mod1, mod2, test = "Chisq")`
- все сразу `drop1(mod1, test = "Chisq")`


## Решение (шаг 1)

```{r purl=FALSE}
drop1(GLM1, test = "Chisq")
# Нужно убрать lcp
```

## Решение (шаг 2)

```{r purl=FALSE}
# Убираем lcp
GLM2 <- update(GLM1, . ~ . - lcp)
drop1(GLM2, test = "Chisq")
# Нужно убрать lbph
```

## Решение (шаг 3)

```{r purl=FALSE}
# Убираем lbph
GLM3 <- update(GLM2, . ~ . - lbph)
drop1(GLM3, test = "Chisq")
# Нужно убрать age
```

## Решение (шаг 4)

```{r purl=FALSE}
# Убираем age
GLM4 <- update(GLM3, . ~ . - age)
drop1(GLM4, test = "Chisq")
# Нужно убрать pgg45
```
## Решение (шаг 4)

```{r purl=FALSE}
# Убираем pgg45
GLM5 <- update(GLM4, . ~ . - pgg45)
drop1(GLM5, test = "Chisq")
# Больше ничего убрать не получается
```

## Решение (шаг 5)

```{r purl=FALSE}
summary(GLM5)
```

Финальную модель снова нужно проверить на выполнение условий применимости

# Информационные критерии

## AIC - Информационный критерий Акаике (Akaike Information Criterion)

$AIC = -2 logLik + 2p$

- $logLik$ - логарифм правдоподобия для модели
- $2p$ - штраф за введение в модель $p$ параметров

Чем меньше AIC - тем лучше модель


__Важно!__   Информационные критерии можно использовать для сравнения __даже для невложенных моделей__. Но модели должны быть __подобраны с помощью ML__ и __на одинаковых данных__!

## Некоторые другие информационные критерии

|Критерий | Название  | Формула|
|------ | ------ | ------|
|AIC | Информационный критерий Акаике | $AIC = -2 logLik + 2p$|
|BIC | Баесовский информационный критерий | $BIC = -2 logLik + p \cdot ln(n)$|
|AICc | Информационный критерий Акаике с коррекцией для малых выборок (малых относительно числа параметров: $n/p < 40$, Burnham, Anderson, 2004) | $AIC_c = -2 logLik + 2p + \frac{2p(p + 1)}{n - p - 1}$|

- $logLik$ - логарифм правдоподобия для модели
- $p$ - число параметров
- $n$ - число наблюдений


# Информационные критерии в R

## Рассчитаем AIC для наших моделей

```{r}
AIC(GLM1, GLM2, GLM3, GLM4, GLM5)
```

>- Судя по AIC, лучшая модель GLM4, но значения AIC различаются всего на 1-2 единицы --- такими различиями можно пренебречь и выбрать более простую модель GLM5.

## Рассчитаем BIC для наших моделей

```{r purl=FALSE}
BIC(GLM1, GLM2, GLM3, GLM4, GLM5)
```

>- Судя по BIC, нужно выбрать модель GLM5

## Take-home messages

- Модели, которые качественно описывают существующие данные включают много параметров, но предсказания с их помощью менее точны из-за переобучения
- Для выбора оптимальной модели используются разные критерии в зависимости от задачи
- Сравнивая вложенные модели можно отбраковать переменные, включение которых в модель не улучшает ее


## Что почитать

+ <span style="color:red">Must read paper!</span> Zuur, A.F. and Ieno, E.N., 2016. A protocol for conducting and presenting results of regression‐type analyses. Methods in Ecology and Evolution, 7(6), pp.636-645.

+ Кабаков Р.И. R в действии. Анализ и визуализация данных на языке R. М.: ДМК Пресс, 2014
+ Zuur, A., Ieno, E.N. and Smith, G.M., 2007. Analyzing ecological data. Springer Science & Business Media.
+ Quinn G.P., Keough M.J. 2002. Experimental design and data analysis for biologists
+ Logan M. 2010. Biostatistical Design and Analysis Using R. A Practical Guide
