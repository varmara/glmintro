---
title: "Линейная регрессия"
subtitle: 
author: "Вадим Хайтов"
output:
  ioslides_presentation:
    css: assets/my_styles.css
    logo: 
    widescreen: yes
---


## Мы рассмотрим 

- Базовые идеи корреляционного анализа
- Проблему двух статистических подходов: "Тестирование гипотез vs. построение моделей"
- Разнообразие статистических моделей
- Основы регрессионного анализа

### Вы сможете

+ Оценить взаимосвязь между измеренными величинами
+ Объяснить что такое линейная модель
+ Формализовать запись модели в виде уравнения
+ Подобрать модель линейной регрессии
+ Протестировать гипотезы о наличии зависимости при помощи t-критерия или F-критерия
+ Оценить предсказательную силу модели 

```{r setup, include = FALSE, cache = FALSE, purl = FALSE}
# output options
options(width = 70, scipen = 6, digits = 3)
library(knitr)
# chunk default options
opts_chunk$set(fig.align='center', tidy = FALSE, fig.width = 7, fig.height = 3, warning = FALSE)
```

## Пример: Соотношения веса тела и веса сердца у кошек
Связан ли вес целого и вес части

Было исследовано 97 котов 

У каждого индивида измеряли:

- вес тела  (кг), 
- вес сердца (г), 

Пример взят из работы: R. A. Fisher (1947) The analysis of covariance method for the relation between a part and the whole, Biometrics 3, 65–68. 
Данные представлены в пакете `boot` 


## Знакомство с данными

Посмотрим на датасет


```{r}
library(readxl)
cat <- read_excel("data/catsM.xlsx")
head(cat)

```

Есть ли пропущенные значения?

```{r}
sum(!complete.cases(cat))
```

Нет пропусков

##Каков объем выборки

```{r}
nrow(cat) 
```

##Есть ли отскакивающие значения? {.smaller}

```{r fig.height=2}
library(ggplot2)
gg_dot <- ggplot(cat, aes(y = 1:nrow(cat))) + geom_point()
gg_dot + aes(x = Bwt)
gg_dot + aes(x = Hwt)
```

# Корреляционный анализ

*Цель практически любого исследования* - поиск взаимосвязи величин и создание базы для предсказания неизвестного на основе имеющихся данных

## Вспомним: _Сила и направление связи между величинами_

```{r, warning=FALSE, echo = FALSE, fig.align='center', fig.height=4, fig.width=8, message=FALSE, purl=FALSE}
library (ggplot2)
library (gridExtra)
theme_set(theme_bw())
x <- rnorm(100, 10, 5)
y1 <- 5*x + rnorm(100, 0, 5)
pl_pos_cor <- ggplot(data.frame(x = x, y = y1), aes(x = x, y = y)) + geom_point() + xlab("First variable") + ylab("Second variable") + ggtitle("Positive correlation")

y2 <- -5*x + rnorm(100, 0, 5)
pl_neg_cor <- ggplot(data.frame(x = x, y = y2), aes(x = x, y = y)) + geom_point() + xlab("First variable") + ylab("Second variable") + ggtitle("Negative correlation")

y3 <- 0*x + rnorm(100, 0, 5)
pl_zero_cor <- ggplot(data.frame(x = x, y = y3), aes(x = x, y = y)) + geom_point() + xlab("First variable") + ylab("Second variable") + ggtitle("No correlation")

grid.arrange(pl_pos_cor, pl_neg_cor, pl_zero_cor, ncol = 3)

```

## Коэффициенты корреляции и условия их применимости   

Коэффициент | Функция | Особенности применения
|-------------|--------------------------------|-------------|
Коэф. Пирсона | `cor(x,y,method="pearson")` | Оценивает связь двух нормально распределенных величин. Выявляет только линейную составляющую взаимосвязи.
Ранговые коэффициенты (коэф. Спирмена, Кендалла) | `cor(x,y,method="spirman")`<br>`cor(x,y,method="kendall")`   | Не зависят от формы распределения. Могут оценивать связь для любых монотонных зависимостей. 




## Оценка достоверности коэффициентов корреляции

- Коэффициент корреляции - это статистика, значение которой описывает степень взаимосвязи двух сопряженных переменных. Следовательно применима логика статистического критерия. 
- Нулевая гипотеза $H_0: r=0$
- Бывают двусторонние $H_a: r\ne 0$ и односторонние критерии $H_a: r>0$ или $H_a: r<0$
- Ошибка коэффициента Пирсона: $SE_r=\sqrt{\frac{1-r^2}{n-2}}$
- Стандартизованная величина $t=\frac{r}{SE_r}$ подчиняется распределению Стьюдента с параметром $df = n-2$
- Для ранговых коэффициентов существует проблема "совпадающих рангов" (tied ranks), что приводит к приблизительной оценке $r$ и приблизительной оценке уровня значимости. 


##Оценка достоверности корреляции
Для оценки статистической значимости коэффициентов корреляции можно использовать функцию `cor.test()`

## Задание

+ Определите силу и направление связи между исследованными признаками
+ Постройте точечную диаграмму, отражающую взаимосвязь между весом тела и весом сердца
+ Оцените достоверность значения коэффициента корреляции Пирсона между этими двумя переменными 


*Hint* Для построения точечной диаграммы вам понадобится `geom_point()`

## Решение 

```{r, size=2, tidy=TRUE}
cor.test(cat$Bwt, cat$Hwt)

```


## Решение

```{r pl-cat_expose, fig.width=4}
pl_cat <- ggplot(cat, 
               aes(x = Bwt, y = Hwt)) + 
  geom_point() + 
  xlab("Вес кота") + 
  ylab("Вес сердца")
pl_cat
```

##Задание: проанализируйте данные и оцените корреляцию между признаками

Зависимость между диаметром клеток инфузорий и концентрацией клеток в культуре

данные представлены в пакете `ISwR`

```{r}
infus <- read_excel("data/hellung.xls")
```

Опишите связь между концентрацией клеток и их диаметром 


<!-- ## Частные корреляции -->

<!-- Частная корреляция - описывает связь между двумя переменными при условии, что влияние других переменных удалено. -->

<!-- Мы удаляем из $X$ и $Y$ ту часть зависимости, которая вызвана влиянием $Z$    -->

<!-- ## Симулированный пример -->

<!-- <div class="columns-2"> -->

<!-- ```{r, fig.height=5, fig.width=5} -->
<!-- z <-  runif(100, 1, 10) -->
<!-- x <- z*10 + rnorm(100, 0, 1)  -->
<!-- y <- -0.9*x + rnorm(100, 0, 1)+z*10 -->
<!-- qplot(x, y) -->
<!-- ``` -->
<!-- <br> -->
<!-- ```{r} -->
<!-- library(ppcor) -->
<!-- cor.test(y, x) -->
<!-- pcor.test(y, x, z) -->

<!-- ``` -->

<!-- </div> -->


## Ковариация 

В некоторых методах статистики (многомерные методы) вместо корреляции применяется ковариация (согласованное отклонение):

$$
cov(X, Y) = \frac{1}{n - 1}\sum{(x_i - \bar{x})(y_i - \bar{y})}
$$
В отлиие от корреляции - это не стандартизованная величина, варьирующая в неограниченных пределах





##   Два подхода к исследованию: <br> Тестирование гипотезы <br>VS<br> Построение модели 

+ Проведя корреляционный анализ, мы лишь ответили на вопрос "Существует ли статистически значимая связь между величинами?"

+ Сможем ли мы, используя это знание, _предсказть_ значения одной величины, исходя из знаний другой? 

## Тестирование гипотезы VS построение модели 

- Простейший пример  
- Между путем, пройденным автомобилем, и временем, проведенным в движении, несомненно есть связь. Хватает ли нам этого знания?   
- Для расчета величины пути в зависимости от времени необходимо построить модель: $S=Vt$, где $S$ - зависимая величина, $t$ - независимая переменная, $V$ - параметр модели.
- Зная параметр модели (скорость) и значение независимой переменной (время), мы можем рассчитать (*cмоделировать*) величину пройденного пути


# Какие бывают модели?

## Линейные и нелинейные модели
<br>

Линейные модели 
$$y = b_0 + b_1x$$ <br> $$y = b_0 + b_1x_1 + b_2x_2$$ 
Нелинейные модели 
$$y = b_0 + b_1^x$$ <br>  $$y = b_0^{b_1x_1+b_2x_2}$$ 

## Простые и многокомпонентные (множественные) модели

+ Простая модель
 $$y = b_0 + b_1x$$ 

+ Множественная модель
 $$y = b_0 + b_1x_1 + b_2x_2 + b_3x_3 + ... + b_nx_n$$ 


## Детерминистские и стохастические модели

<div class="columns-2">
```{r,echo=FALSE, fig.height=4, fig.width=4, warning=FALSE, purl=FALSE}
x <- 1:20
y <- 2 + 5*x
ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point(size=4)  + geom_abline(slope=5, intercept = 2) + ylim(0, 100) 
```
Модель: $у_i = 2 + 5x_i$    
Два параметра: угловой коэффициент (slope) $b_1=5$; свободный член (intercept) $b_0=2$   
Чему равен $y$ при $x=10$?


```{r,echo=FALSE, fig.height=4, fig.width=4, warning=FALSE, purl=FALSE}
x <- 1:20
y <- 2 + 5*x + rnorm(20,0, 20)
ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) + geom_point(size=4)  + geom_abline(slope=5, intercept = 2)  + ylim(0,100) + theme_bw()

```

Модель: $у_i = 2 + 5x_i + \epsilon_i$    
Появляется дополнительный член $\epsilon_i$ 
Он вводит в модель влияние неучтенных моделью факторов. 
Обычно считают, что $\epsilon \in N(0, \sigma^2)$ 


</div>


## Модели с дискретными предикторами

```{r , echo=FALSE}
set.seed(1234)
x <- data.frame(labels = c(rep("Level 1", 10), rep( "Level 2", 10), rep("Level 3", 10)), response = c(rnorm(10, 5, 1), rnorm(10, 10, 1), rnorm(10, 15, 1))) 

ggplot(x, aes(x=labels, y=response)) + geom_boxplot()+ geom_point(color="blue", size=4) + xlab(" ") 

```

Модель для данного примера имеет такой вид  
<br>
<br>
$response = 4.6 + 5.3I_{Level2} + 9.9 I_{Level3} + \epsilon_i$

$I_{i}$ - dummy variable   


## Случайная и фиксированая часть модели
В стохастические модели выделяется две части:

**Фиксированная часть:** $у_i = 2 + 5x_i$   
**Случайная часть:** $\epsilon_i$ 

Бывают модели, в которых случайная часть выглядит существенно сложнее (модели со смешаными эффектами). В таких моделях необходимо смоделировать еще и поведение случайной части.



## Модель для зависимости массы сердца от веса тела

Какая из линий "лучше" описывает облако точек?

```{r, echo=FALSE, fig.align='center', fig.height= 5, fig.width=7}
library(ggplot2)

pl_1 <- pl_cat + geom_smooth(method = "lm", se = FALSE, size=2) + geom_abline(slope = 4.31, intercept = -1.18, color="green", size = 2) + geom_abline(slope = 4.8, intercept = -1.2, color="red", size=2) 

grid.arrange (pl_cat, pl_1, ncol=2)

```




# Найти оптимальную модель позволяет регрессионный анализ
<div  align="left">

"Essentially, all models are wrong,     
but some are useful"     
(Georg E. P. Box) 

</div>

## Происхождение термина "регрессия"

<div class="columns-2">


<img src="images/Galton.png" width="220" height="299" >


Френсис Галтон (Francis Galton)


"the Stature of the adult offspring … [is] … more mediocre than the
stature of their Parents" (цит. по `Legendre & Legendre, 1998`)

Рост _регрессирует_ (возвращается) к популяционной средней     
Угловой коэффициент в зависимости роста потомков от роста родителей- _коэффциент регресси_


</div>



## Подбор линии регрессии проводится с помощью двух методов 

>- С помощью метода наименьших квадратов (Ordinary Least Squares) - используется для простых линейных моделей
<br>

>- Через подбор функции максимального правдоподобия (Maximum Likelihood) - используется для подгонки сложных линейных и нелинейных моделей.


## Кратко о методе макcимального правдоподобия 
```{r gg-norm-tunnel, echo=FALSE, fig.height=7, purl=FALSE}
## Based on code by Arthur Charpentier:
## http://freakonometrics.hypotheses.org/9593
## TODO: wrap it into a function and adapt it for use with other distributions
## as Markus Gesmann has done here
## http://www.magesblog.com/2015/08/visualising-theoretical-distributions.html

n <- 2
X <- cat$Bwt 
Y <- cat$Hwt
df <- data.frame(X,Y)

# regression
reggig <- glm(Y ~ X, data = df, family = gaussian(link = "identity"))

# empty plot
vX <- seq(min(X) - 0.1, max(X) + 0.1, length = n)
vY <- seq(min(Y) - 10, max(Y) + 10, length = n)
mat <- persp(x = vX, y = vY, z = matrix(0, n, n), 
             zlim = c(0, 0.1),
             theta =  - 30, phi = 20, expand = 0.1,
             ticktype  = "detailed",  box = FALSE, border = "gray60")

x <- seq(min(X), max(X), length = 501)

# expected values
C <- trans3d(x, predict(reggig, newdata = data.frame(X = x), type = "response"), rep(0, length(x)), mat)
lines(C, lwd = 2)

sdgig <- sqrt(summary(reggig)$dispersion)

# 1SD
y1 <- qnorm(.95, predict(reggig, newdata = data.frame(X = x), type = "response"),  sdgig)
C <- trans3d(x, y1, rep(0, length(x)), mat)
lines(C, lty = 2, col = "#d95f02")
y2 <- qnorm(.05, predict(reggig, newdata = data.frame(X = x), type = "response"),  sdgig)
C <- trans3d(x, y2, rep(0, length(x)), mat)
lines(C, lty = 2, col = "#d95f02")

# C <- trans3d(c(x, rev(x)), c(y1, rev(y2)), rep(0, 2 * length(x)), mat)
# polygon(C, border = NA, col = "yellow")

# data points
C <- trans3d(X, Y, rep(0, length(X)), mat)
points(C, pch = 1, col = "black", cex = 0.4)

# density curves
n <- 6
vX <- seq(min(X), max(X), length = n)

mgig <- predict(reggig, newdata = data.frame(X = vX))

sdgig <- sqrt(summary(reggig)$dispersion)

for(j in n:1){
  stp <- 251
  x <- rep(vX[j], stp)
  y <- seq(min(min(Y) - 10, 
               qnorm(.05, 
                     predict(reggig, 
                             newdata = data.frame(X = vX[j]), 
                             type = "response"),  
                     sdgig)), 
           max(Y) + 10, 
           length = stp)
  z0 <- rep(0, stp)
  z <- dnorm(y,  mgig[j],  sdgig)
  C <- trans3d(c(x, x), c(y, rev(y)), c(z, z0), mat)
  polygon(C, border = NA, col = "light blue", density = 40)
  C <- trans3d(x, y, z0, mat)
  lines(C, lty = 2, col = "grey60")
  C <- trans3d(x, y, z, mat)
  lines(C, col = "steelblue")
}
```



## Кратко о методе макcимального правдоподобия 
Симулированный пример с использованием `geom_violin()`
```{r, echo=FALSE, fig.height=6}

xy <- data.frame(X = rep(1:10, 3))
xy$Y <- 10*xy$X + rnorm(30, 0, 10)
xy$predicted <- predict(lm(Y ~ X, data = xy))


rand_df <- matrix(rep(NA,100000), ncol = 10)
for(i in 1:10) rand_df[,i] <- rnorm(10000, xy$predicted[i], 10)

rand_df <- data.frame(X = rep(xy$X, each = 10000), Y = as.vector(rand_df))



ggplot(xy, aes(x = X, y = Y)) + geom_violin(data = rand_df, aes(x = factor(X)), scale = ) + geom_point() + geom_smooth(method = "lm", se = F) + geom_point(data = xy, aes(x = X, y = predicted), color = "red", size = 3) + labs(x = "Предиктор", y = "Зависимая переменная") 


```
  

## Метод наименьших квадратов

<div class="columns-2">
<img src="images/OLS.png" width="500" height="400" >

<div class = "footnote">
(из кн. Quinn, Keough, 2002, стр. 85)     
</div>


Остатки (Residuals):            
$$e_i = y_i - \hat{y_i}$$

Линия регрессии (подобранная модель) - это та линия, у которой $\sum{e_i}^2$ минимальна.



## Подбор модели методом наменьших квадратов с помощью функци `lm()`  
`fit <- lm(formula, data)`

Модель записывается в виде формулы  

Модель | Формула
|-------------|-------------|  
Простая линейная регрессия <br>$\hat{y_i}=b_0 + b_1x_i$ | `Y ~ X` <br> `Y ~ 1 + X` <br> `Y ~ X + 1`  
Простая линейная регрессия <br> (без $b_0$, "no intercept") <br> $\hat{y_i}=b_1x_i$ | `Y ~ -1 + X` <br> `Y ~ X - 1`  
Уменьшенная простая линейная регрессия <br> $\hat{y_i}=b_0$ | `Y ~ 1` <br> `Y ~ 1 - X`  
Множественная линейная регрессия <br> $\hat{y_i}=b_0 + b_1x_i +b_2x_2$ | `Y ~ X1 + X2`  



## Подбор модели методом наменьших квадратов с помощью функци `lm()` {.smaller}
`fit <- lm(formula, data)`

Элементы формул для записи множественных моделей

Элемент формулы | Значение 
|-------------|-------------| 
`:` | Взаимодействие предикторов <br> `Y ~ X1 + X2 + X1:X2`
`*` | Обозначает полную схему взаимодействий <br>  `Y ~ X1 * X2 * X3` <br> аналогично <br> `Y ~ X1 + X2 + X3+ X1:X2 + X1:X3 + X2:X3 + X1:X2:X3` 
`.` | `Y ~ .` <br> В правой части формулы записываются все переменные из датафрейма, кроме `Y` 


## Подберем модель, наилучшим образом описывающую зависимость массы сердца от веса тела

```{r cat-mod, purl=FALSE}
cat_model <- lm(Hwt ~ Bwt, data = cat)
cat_model
```


## Как трактовать значения параметров регрессионной модели?

```{r, echo=FALSE, warning=FALSE, fig.align='center',fig.width=9, fig.height=5}
n=100
x <- rnorm(n, 10, 5)
y1 <- 5*x + 50 + rnorm(n, 0, 2)
y2 <- -5*x + 50 + rnorm(n, 0, 2)
y3 <- 0*x + 50 + rnorm(n, 0, 2)
label <- c(rep("Positive slope",n), rep("Negative slope", n), rep("Zero slope", n))
df1 <- data.frame(x = rep(x, 3), y = c(y1, y2, y3), label = label)
df1a <- data.frame(intercept = c(50, 50, 50), slope = c(-5,0,5))
pl_1 <- ggplot(data = df1, aes(x = x, y = y, color = label)) + geom_point() + xlab("Independent (X)") + ylab("Dependent (Y)") + xlim(0, 25) + guides(color=F) + geom_abline(data = df1a, aes(intercept = intercept, slope = slope), size=1) + ggtitle("Constant intercepts \n Different slopes")

x <- rnorm(n, 10, 5)
y1 <- 5*x + 0 + rnorm(n, 0, 2)
y2 <- 5*x + 30 + rnorm(n, 0, 2)
y3 <- 5*x + 60 + rnorm(n, 0, 2)
label <- c(rep("Intercept = 0",n), rep("Intercept = 30", n), rep("Intercept = 60", n))
df2 <- data.frame(x = rep(x, 3), y = c(y1, y2, y3), label = label)
df2a <- data.frame(intercept=c(30, 0, 60), slope=c(5, 5, 5))
pl_2 <- ggplot(df2, aes(x = x, y = y, color=label)) + geom_point() + xlab("Independent (X)") + ylab("Dependent (Y)") + xlim(0, 25) + guides(color=F) + geom_abline(data = df2a, aes(intercept = intercept, slope = slope), size=1) + ggtitle("Different intercepts \n Constant slopes")


x <- rnorm(n, 10, 5)
y1 <- 0*x + 0 + rnorm(n, 0, 2)
y2 <- 0*x + 30 + rnorm(n, 0, 2)
y3 <- 0*x + 60 + rnorm(n, 0, 2)
label <- c(rep("Intercept = 0",n), rep("Intercept = 30", n), rep("Intercept = 60", n))
df3 <- data.frame(x = rep(x, 3), y = c(y1, y2, y3), label = label)
df3a <- data.frame(intercept = c(30, 0, 60), slope = c(0, 0, 0))
pl_3 <- ggplot(data = df3, aes(x = x, y = y, color=label)) + geom_point() + xlab("Independent (X)") + ylab("Dependent (Y)") + xlim(0, 25) + guides(color=F) + geom_abline(data = df3a, aes(intercept = intercept, slope = slope), size=1) + ggtitle("Different intercepts \n Zero slopes")

grid.arrange(pl_1, pl_2, pl_3, nrow=1)

```


## Как трактовать значения параметров регрессионной модели?

>- Угловой коэффициент (_slope_) показывает на сколько _единиц_ изменяется предсказанное значение $\hat{y}$ при изменении на _одну единицу_ значения предиктора ($x$)

>- Свободный член (_intercept_) - величина во многих случаях не имеющая "смысла", просто поправочный коэффициент, без которого нельзя вычислить $\hat{y}$. _NB!_ В некоторых линейных моделях он имеет смысл, например, значения $\hat{y}$ при $x = 0$. 

>- Остатки (_residuals_) - характеризуют влияние неучтенных моделью факторов.

## Вопросы: 
1. Чему равны угловой коэффициент и свободный член полученной модели `cat_model`?       
2. Какое значение веса сердца предсказывает модель для кота весом 2.5 кг         
3. Чему равно значение остатка от модели для кота с порядковым номером 10?    

## Ответы
```{r}
coefficients(cat_model) [1]
coefficients(cat_model) [2]

```


## Ответы
```{r}
as.numeric(coefficients(cat_model) [1] + coefficients(cat_model) [2] * 2.5)

```

## Ответы
```{r}
cat$Hwt[10] - fitted(cat_model)[10]
residuals(cat_model)[10]

```



## Углубляемся в анализ модели: функция `summary()`
```{r}
summary(cat_model)

```


## Что означают следующие величины?

`Estimate`  
`Std. Error`   
`t value`  
`Pr(>|t|)`   


## Оценки параметров регрессионной модели

Параметр | Оценка      | Стандартная ошибка   
|-------------|--------------------|-------------|   
$\beta_1$ <br> Slope| $b _1 = \frac {\sum _{i=1}^{n} {[(x _i - \bar {x})(y _i - \bar {y})]}}{\sum _{i=1}^{n} {(x _i - \bar x)^2}}$<br> или проще <br> $b_0 = r\frac{sd_y}{sd_x}$ | $SE _{b _1} = \sqrt{\frac{MS _e}{\sum _{i=1}^{n} {(x _i - \bar {x})^2}}}$   
$\beta_0$ <br> Intercept | $b_0 = \bar y - b_1 \bar{x}$  | $SE _{b _0} = \sqrt{MS _e [\frac{1}{n} + \frac{\bar x}{\sum _{i=1}^{n} {(x _i - \bar x)^2}}]}$   
$\epsilon _i$ | $e_i = y_i - \hat {y_i}$ | $\approx \sqrt{MS_e}$   



## Для чего нужны стандартные ошибки?
>- Они нужны, поскольку мы _оцениваем_ параметры по _выборке_
>- Они позволяют построить доверительные интервалы для параметров
>- Их используют в статистических тестах


## Графическое представление результатов {.columns-2}

```{r, fig.height=5, fig.width=5}
pl_cat + geom_smooth(method="lm") 
```

<br>
<br>
<br>
Доверительная зона регрессии. В ней с 95% вероятностью лежит регрессионная прямая, описывающая связь в генеральной совокупности. <br>
Возникает из-за неопределенности оценок коэффициентов модели, вследствие выборочного характера оценок.           


## Симулированный пример

Линии регрессии, полученные для 100 выборок (по 20 объектов в каждой), взятых из одной и той же генеральной совокупности 
```{r, echo=FALSE, fig.align='center', fig.height=5, purl = FALSE}
pop_x <- rnorm(1000, 10, 3)
pop_y <- 10 + 10*pop_x + rnorm(1000, 0, 20)
population <- data.frame(x=pop_x, y=pop_y)
samp_coef <- data.frame(b0 = rep(NA, 100), b1=rep(NA, 100))
for(i in 1:100) {
  samp_num <- sample(1:1000, 20)
  samp <- population[samp_num, ]
  fit <- lm(y~x, data=samp)
  samp_coef$b0[i] <- coef(fit)[1]
  samp_coef$b1[i] <- coef(fit)[2]
  
 }

ggplot(population, aes(x=x, y=y)) + geom_point(alpha=0.3, color="red")+ geom_abline(aes(intercept=b0, slope=b1), data=samp_coef) + geom_abline(aes(intercept=10, slope=10), color="blue", size=2)
```



## Доверительные интервалы для коэффициентов уравнения регрессии

```{r}
coef(cat_model)

confint(cat_model)
```

## Для разных $\alpha$ можно построить разные доверительные интервалы

```{r , echo=FALSE, fig.align='center', fig.height=5, fig.width=9}
pl_alpha1 <- pl_cat + geom_smooth(method="lm", level=0.8) + ggtitle(bquote(alpha==0.2))

pl_alpha2 <- pl_cat + geom_smooth(method="lm", level=0.95) + ggtitle(bquote(alpha==0.05))

pl_alpha3 <- pl_cat + geom_smooth(method="lm", level=0.999) + ggtitle(bquote(alpha==0.01))


grid.arrange(pl_alpha1, pl_alpha2, pl_alpha3, ncol=3)

```

## Важно!

Если коэффициенты уравнения регрессии - лишь приблизительные оценки параметров, то предсказать значения зависимой переменной можно только _с нeкоторой вероятностью_.           

## Какое значение веса сердца можно ожидать у кота с весом 2.5 кг?

```{r, tidy=TRUE}
newdata <- data.frame(Bwt = 2.5)

Predicted <- predict(cat_model, newdata, interval = "prediction", 
        level = 0.95, se = TRUE)$fit
Predicted
```

>- При весе тела 2.5 кг среднее значение веса сердца будет, с вероятностью 95%, находиться в интервале от `r Predicted[2]`  до  `r Predicted[3]` 



## Отражаем на графике область значений, в которую попадут 95% предсказанных величин IQ

Подготавливаем данные

```{r , warning=FALSE}
cat_predicted <- predict(cat_model, interval="prediction")
cat_predicted <- data.frame(cat, cat_predicted)
head(cat_predicted)
```


## Отражаем на графике область значений, в которую попадут 95% предсказанных величин 

```{r pl-predict, echo=FALSE, fig.align='center', fig.height=5}
pl_cat + 

# 1) Линия регрессии и ее дов. интервал
# Если мы указываем fill внутри aes() и задаем фиксированное значение - появится соотв. легенда с названием.
# alpha - задает прозрачность
  geom_smooth(method = "lm", aes(fill = "Conf.interval"), alpha = 0.4, size = 2) +
# 2) Интервал предсказаний создаем при помощи геома ribbon ("лента")
# Данные берем из другого датафрейма - из cat_predicted
# ymin и ymax - эстетики геома ribbon, которые задают нижний и верхний край ленты в точках с заданным x (x = MRINACount было задано в ggplot() при создании pl_cat, поэтому сейчас его указывать не обязательно)
  geom_ribbon(data = cat_predicted,  aes(ymin = lwr, ymax = upr, fill = "Conf. area for prediction"), alpha = 0.2) +

# 3) Вручную настраиваем цвета заливки при помощи шкалы fill_manual.
# Ее аргумент name - название соотв. легенды, values - вектор цветов
  scale_fill_manual(name = "Intervals", values = c("green", "blue")) +

# 4) Название графика
  ggtitle("Confidence interval \n and confidence area for prediction")
```



## Важно!

<dev class="columns-2">
*Модель "работает" только в том диапазоне значений независимой переменной ($x$), для которой она построена (интерполяция). Экстраполяцию надо применять с большой осторожностью.*

```{r, fig.align='center', fig.height=5, fig.width=9, echo=FALSE}
pl_cat + 
  geom_ribbon(data=cat_predicted, aes(y=fit, ymin=lwr, ymax=upr, fill = "Conf. area for prediction"), alpha=0.2) + 
  geom_smooth(method="lm", aes(fill="Conf.interval"), alpha=0.4) + 
  scale_fill_manual("Intervals", values = c("green", "gray")) + 
  ggtitle("Confidence interval \n and confidence area for prediction")+ xlim (1, 5) + geom_text(label="Interpolation", aes(x=2.9, y=11.9)) + geom_text(label="Extrapolation", aes(x=1.5, y=11.9)) +
geom_text(label="Extrapolation", aes(x=4.5, y=11.9))

``` 

</dev>


## Итак, что означают следующие величины?

>- `Estimate` 
>- Оценки праметров регрессионной модели 
>- `Std. Error`   
>- Стандартная ошибка для оценок    
>- Осталось решить, что такое `t value`, `Pr(>|t|)`


## Тестирование гипотез с помощью линейных моделей
 
### Два равноправных способа
>- Проверка достоверности оценок коэффициента $b_1$ (t-критерий). 
>- Оценка соотношения описанной и остаточной дисперсии (F-критерий). 

## Тестирование гипотез с помощью t-критерия  

Зависимость есть, если $\beta_1 \ne 0$ 

Нулевая гипотеза $H_0: \beta = 0$

Тестируем гипотезу 

$$t=\frac{b_1-0}{SE_{b_1}}$$

Число степеней свободы: $df=n-2$     
>- Итак,           
>- `t value` - Значение t-критерия          
>- `Pr(>|t|)` - Уровень значимости         


## Зависит ли вес сердца  от веса тела ? 

$$Hwt = -1.18 + 4.31 Bwt$$

```{r}
summary(cat_model)
```

## Тестирование гипотез с помощью F-критерия  

```{r echo=FALSE, fig.height=2.8, fig.width=10}

pl_exp <- pl_cat + geom_smooth(method="lm", se=F, size=1.3) + geom_abline(aes(intercept=mean(Hwt), slope=0), size=1.3) + geom_text(label="Mean Hwt", aes(x=1.5, y=(mean(Hwt)-1)), size = 4) + geom_segment(data=cat_predicted, aes(x=Bwt, y=mean(Hwt), xend=Bwt, yend=fit)) + ggtitle("Explained variation") + xlim(1, 4)

 pl_res <- pl_cat + geom_smooth(method="lm", se=F, size=1.3) + geom_segment(data=cat_predicted, aes(x=Bwt, y=Hwt, xend=Bwt, yend=fit)) + ggtitle("Residual variation") + xlim(1, 4)

pl_tot <-pl_cat + geom_abline(aes(intercept=mean(Hwt), slope=0), size=1.3) + geom_text(label="Mean Hwt", aes(x=1.5, y=(mean(Hwt)-1)), size = 5) + geom_segment(data=cat_predicted, aes(x=Bwt, y=Hwt, xend=Bwt, yend=mean(Hwt))) + ggtitle("Total variation") + xlim(1, 4)


grid.arrange(pl_exp, pl_res, pl_tot, nrow=1)
```



|**Объясненная дисперсия**  | **Остаточная дисперсия**   | **Полная дисперсия** | 
|-----|-----|-----|
| $SS_{Regression}=\sum{(\hat{y}-\bar{y})^2}$ | $SS_{Residual}=\sum{(\hat{y}-y_i)^2}$ | $SS_{Total}=\sum{(\bar{y}-y_i)^2}$ |
| $df_{Regression} = 1$ | $df_{Residual} = n-2$  | $df_{Total} = n-1$ |
| $MS_{Regression} =\frac{SS_{Regression}}{df}$ | $MS_{Residual} =\frac{SS_{Residual}}{df_{Residual}}$ | $MS_{Total} =\frac{SS_{Total}}{df_{Total}}$ |

## F критерий

Если зависимости нет, то $MS _{Regression} = MS_{Residual}$

 $$ F= \frac{MS _{Regression}}{MS_{Residual}}$$

Логика та же, что и с t-критерием  


```{r, echo=FALSE, fig.width=5}
f <- seq(-0.2, 10, 0.1)
ggplot(data.frame(f = f, p = df(f, 1, 38)), aes(x = f, y = p)) + geom_line(size = 1.3) + ggtitle("F-distribution") + xlab("F") + geom_vline(xintercept = c(6.686), color = "red",) + geom_hline(yintercept = 0) + xlim(-0.2, 10)
```

Форма F-распределения зависит от двух параметров:
$df_{Regression} = 1$ и $df_{Residual} = n-2$



## Оценка качества подгонки модели с помощью коэффициента детерминации

### В чем различие между этми двумя моделями?

```{r, echo=FALSE, fig.align='center', fig.height=5}
x <- rnorm(100, 20, 5)
y1 <- 10 * x + 5 + rnorm(100, 0, 5)
y2 <- 10 * x + 5 + rnorm(100, 0, 40)
d <- data.frame(x=x, y1=y1)
pl_R1 <- ggplot(d, aes(x=x, y=y1)) + geom_point() + geom_smooth(method="lm", se=F) 
pl_R2 <- ggplot(d, aes(x=x, y=y2)) + geom_point() + geom_smooth(method="lm", se=F) 
grid.arrange (pl_R1, pl_R2)
```

## Оценка качества подгонки модели с помощью коэффициента детерминации

Коэффициент детерминации описывает какую долю дисперсии зависимой переменной объясняет модель

>- $$R^2 = \frac{SS_{Regression}}{SS_{Total}}$$
>- $$0< R^2 < 1$$
>- В случае с простой линейной моделью $$R^2 = r^2$$


## Еще раз смотрим на результаты регрессионного анализа зависимости веса сердца от веса тела

```{r}
summary(cat_model)
```

## Adjusted R-squared - скорректированный коэффициет детерминации

Применяется если необходимо сравнить две модели с разным количеством параметров  

$$ R^2_{adj} = 1- (1-R^2)\frac{n-1}{n-k}$$

$k$ - количество параметров в модели   

Вводится штраф за каждый новый параметр

## Как записываются результаты регрессионного анлиза в тексте статьи?

Мы показали, что связь между весом сердца и весом тела котов описывается моделью вида
<br>
Hwt = -1.18 + 4.31 Bwt ($F_{1,95}$ = 161, p < 0.001, $R^2$ = 0.63)
<br>
<br>



## Summary
> - Модель простой линейной регрессии $y _i = \beta _0 + \beta _1 x _i + \epsilon _i$
- Параметры модели оцениваются на основе выборки
- В оценке коэффициентов регрессии и предсказанных значений существует неопределенность: необходимо вычислять доверительный интервал. 
- Доверительные интервалы можно расчитать, зная стандартные ошибки.  
- Гипотезы о наличии зависимости можно тестировать при помощи t- или F-теста. $(H _0: \beta _1 = 0$)
- Качество подгонки модели можно оценить при помощи коэффициента детерминации $(R^2)$

## Что почитать

- Гланц, 1999, стр. 221-244
- [Open Intro to Statistics](https://docs.google.com/viewer?docex=1&url=http://www.openintro.org/stat/down/OpenIntroStatSecond.pdf): [Chapter 7. Introduction to linear regression](https://docs.google.com/viewer?docex=1&url=http://www.openintro.org/stat/down/oiStat2_07.pdf), pp. 315-353.  
- Quinn, Keough, 2002, pp. 78-110

